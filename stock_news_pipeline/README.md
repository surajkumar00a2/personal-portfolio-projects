# ğŸ“Š Automated Stock & News Data Pipeline

> **Production-ready ETL pipeline** extracting stock prices and financial news from Yahoo Finance, orchestrated with Apache Airflow

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Airflow-2.7+-red.svg)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

**Author:** Suraj Kumar | [LinkedIn](https://linkedin.com/in/suraj-kumar-0700ba193) | [GitHub](https://github.com/surajkumar00a2) | surajkumar00a2@gmail.com

---

## ğŸ¯ Overview

A comprehensive data engineering project demonstrating ETL pipeline development, workflow orchestration, and data quality engineering. This pipeline extracts historical stock prices and financial news, validates data quality, and stores it in PostgreSQLâ€”all orchestrated by Apache Airflow with monitoring and retry logic.

**Built for:** Portfolio demonstration, interview discussions, and real-world data engineering practices

---

## âœ¨ Key Features

- ğŸ”„ **Automated ETL Pipeline**: Daily extraction of stock prices (OHLC) and financial news
- ğŸ“Š **Data Quality Checks**: Schema validation, null detection, duplicate prevention, row count verification
- ğŸ”§ **Apache Airflow Orchestration**: Scheduled DAGs with task dependencies and retry logic
- ğŸ˜ **PostgreSQL Storage**: Normalized database schema with proper indexing
- ğŸ³ **Dockerized Setup**: One-command deployment with Docker Compose
- ğŸ“ **Comprehensive Logging**: Centralized logging visible in Airflow UI
- âš¡ **Failure Handling**: Automatic retries (3 attempts, 5-minute delays)
- ğŸ“ˆ **Production-Ready**: Follows best practices for reliability and observability

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    A[Yahoo Finance API] --> B[Stock Extractor<br/>yfinance]
    A --> C[News Scraper<br/>BeautifulSoup]
    B --> D[Pandas Transformations]
    C --> D
    D --> E[Data Quality Checks<br/>Nulls, Duplicates, Schema]
    E --> F[(PostgreSQL<br/>stock_prices & stock_news)]
    G[Apache Airflow DAG] -.->|Orchestrates| B
    G -.->|Orchestrates| C
    G -.->|Orchestrates| D
    G -.->|Orchestrates| E
    G -.->|Monitors| F
```

**Pipeline Flow:**
1. **Extract**: Fetch stock data (yfinance) and news (Yahoo Finance RSS)
2. **Transform**: Clean, validate, and enrich data using Pandas
3. **Quality Check**: Run automated validation (nulls, duplicates, schema)
4. **Load**: Store in PostgreSQL with proper error handling
5. **Monitor**: Track execution in Airflow UI with detailed logs

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.9+ |
| **Orchestration** | Apache Airflow 2.7+ |
| **Database** | PostgreSQL 13+ |
| **Data Processing** | Pandas, NumPy |
| **Web Scraping** | BeautifulSoup4, Requests |
| **Stock Data API** | yfinance |
| **Containerization** | Docker, Docker Compose |
| **Environment Management** | python-dotenv |

---

## ğŸ“ Project Structure

```
stock_news_pipeline/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ stock_pipeline_dag.py      # Main Airflow DAG
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ stock_extractor.py         # Stock price extraction
â”‚   â”‚   â”œâ”€â”€ news_scraper.py            # News scraping logic
â”‚   â”‚   â”œâ”€â”€ data_processor.py          # Data cleaning & validation
â”‚   â”‚   â”œâ”€â”€ db_manager.py              # PostgreSQL operations
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration settings
â”‚   â”‚   â””â”€â”€ logger.py                  # Logging setup
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â””â”€â”€ schema.sql                 # Database schema
â”‚   â”œâ”€â”€ Dockerfile                     # Airflow container config
â”‚   â”œâ”€â”€ docker-compose.yml             # Multi-container setup
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ .env.example                   # Environment template
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ .gitignore                         # Git ignore rules
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker** & **Docker Compose** installed ([Get Docker](https://docs.docker.com/get-docker/))
- **Git** installed
- 4GB RAM minimum
- Internet connection for Yahoo Finance API

### Installation

**1. Clone the repository**
```bash
git clone https://github.com/surajkumar00a2/personal-portfolio-projects.git
cd personal-portfolio-projects/stock_news_pipeline
```

**2. Set up environment variables**
```bash
cp airflow/.env.example airflow/.env
```

Edit `airflow/.env` with your settings:
```env
# PostgreSQL Configuration
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=stock_data
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Airflow Configuration
AIRFLOW_UID=50000
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False

# Pipeline Configuration
STOCK_SYMBOLS=AAPL,GOOGL,MSFT,AMZN,TSLA
SCHEDULE_INTERVAL=0 18 * * *  # Daily at 6 PM IST
```

**3. Start the pipeline**
```bash
cd airflow
docker-compose up -d
```

This starts:
- âœ… Apache Airflow (webserver, scheduler)
- âœ… PostgreSQL database
- âœ… All dependencies

**4. Access Airflow UI**
```
URL: http://localhost:8080
Username: airflow
Password: airflow
```

**5. Trigger the pipeline**

**Option A:** Via Airflow UI
- Navigate to DAGs page
- Find `stock_news_pipeline`
- Click the â–¶ï¸ play button

**Option B:** Via CLI
```bash
docker exec -it airflow-webserver airflow dags trigger stock_news_pipeline
```

---

## ğŸ“Š Database Schema

### `stock_prices` Table
```sql
CREATE TABLE stock_prices (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(10, 2),
    high DECIMAL(10, 2),
    low DECIMAL(10, 2),
    close DECIMAL(10, 2),
    volume BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_stock_date UNIQUE (symbol, date)
);
```

### `stock_news` Table
```sql
CREATE TABLE stock_news (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    headline TEXT NOT NULL,
    url TEXT,
    published_date TIMESTAMP,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸ“ˆ Sample Output

**Stock Prices (2,500+ rows per run):**
```sql
SELECT symbol, date, close 
FROM stock_prices 
ORDER BY date DESC 
LIMIT 5;
```
| symbol | date       | close   |
|--------|------------|---------|
| AAPL   | 2024-12-20 | 245.32  |
| GOOGL  | 2024-12-20 | 178.45  |
| MSFT   | 2024-12-20 | 425.67  |

**Stock News (100+ articles per run):**
```sql
SELECT symbol, headline, published_date 
FROM stock_news 
ORDER BY published_date DESC 
LIMIT 3;
```

---

## ğŸ” Data Quality Checks

The pipeline runs comprehensive validation:

| Check | Description |
|-------|-------------|
| **Schema Validation** | Ensures correct column types and structure |
| **Null Detection** | Flags missing critical fields (symbol, date, close) |
| **Duplicate Prevention** | Removes duplicate (symbol, date) entries |
| **Row Count Verification** | Validates expected data volume |
| **Anomaly Detection** | Logs unusual patterns for manual review |

**View Logs:**
```bash
# Check Airflow task logs
docker logs airflow-scheduler

# Check PostgreSQL logs
docker logs postgres
```

---

## âš™ï¸ Configuration

### Customize Stock Symbols
Edit `airflow/.env`:
```env
STOCK_SYMBOLS=AAPL,GOOGL,MSFT,AMZN,TSLA,NFLX,META
```

### Change Schedule
Modify in `airflow/.env` (Cron format):
```env
SCHEDULE_INTERVAL=0 18 * * *  # Daily at 6 PM
# Or: 0 */6 * * * for every 6 hours
```

### Adjust Retry Logic
Edit `airflow/dags/stock_pipeline_dag.py`:
```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}
```

---

## ğŸ§ª Validation & Testing

**Check Pipeline Status:**
```bash
# List all DAGs
docker exec -it airflow-webserver airflow dags list

# Check DAG runs
docker exec -it airflow-webserver airflow dags list-runs -d stock_news_pipeline
```

**Verify Data in PostgreSQL:**
```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U airflow -d stock_data

# Check row counts
SELECT COUNT(*) FROM stock_prices;
SELECT COUNT(*) FROM stock_news;

# View latest data
SELECT * FROM stock_prices ORDER BY date DESC LIMIT 10;
```

---

## ğŸ›‘ Stopping & Cleanup

**Stop services:**
```bash
docker-compose down
```

**Remove all data (caution!):**
```bash
docker-compose down -v  # Removes volumes/database
```

---

## ğŸ¯ Use Cases

- **Portfolio Project**: Demonstrates end-to-end data engineering skills
- **Interview Discussions**: Shows ETL, orchestration, and data quality knowledge
- **Financial Analysis**: Historical data for backtesting trading strategies
- **Learning**: Understand production-ready pipeline design patterns

---

## ğŸš§ Future Enhancements

- [ ] Add real-time streaming with Apache Kafka
- [ ] Implement sentiment analysis on news headlines
- [ ] Create Streamlit dashboard for visualization
- [ ] Add data versioning with DVC
- [ ] Implement CI/CD with GitHub Actions
- [ ] Add unit tests and integration tests
- [ ] Deploy to cloud (AWS/GCP) with Terraform
- [ ] Add more data sources (NSE, BSE for Indian markets)

---

## ğŸ“ Technical Highlights

**Why This Project Stands Out:**
- âœ… **Production Patterns**: Retry logic, logging, error handling
- âœ… **Scalable Design**: Modular code, easy to extend
- âœ… **Best Practices**: Type hints, docstrings, configuration management
- âœ… **Real-World Skills**: Orchestration, data quality, containerization
- âœ… **Interview-Ready**: Can explain architecture, trade-offs, and improvements

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the [issues page](https://github.com/surajkumar00a2/personal-portfolio-projects/issues).

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Suraj Kumar**
- **Role:** Analytics Engineer | Data Engineering
- **GitHub:** [@surajkumar00a2](https://github.com/surajkumar00a2)
- **LinkedIn:** [Suraj Kumar](https://linkedin.com/in/suraj-kumar-0700ba193)
- **Email:** surajkumar00a2@gmail.com

---

## ğŸ™ Acknowledgments

- [Apache Airflow](https://airflow.apache.org/) - Workflow orchestration
- [yfinance](https://github.com/ranaroussi/yfinance) - Yahoo Finance data access
- [Docker](https://www.docker.com/) - Containerization platform

---

<div align="center">

â­ **If you found this project helpful, please consider giving it a star!** â­

</div>

---

> ğŸ’¡ **Note:** This project focuses on reliability, observability, and real-world failure handling rather than toy examples. It's designed to showcase production-ready data engineering practices suitable for portfolio and interview discussions.
